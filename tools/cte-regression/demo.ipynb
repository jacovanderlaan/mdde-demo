{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTE Regression Analysis - DuckDB Demo\n",
    "\n",
    "This notebook demonstrates the CTE regression analysis tools using DuckDB as the backend.\n",
    "DuckDB is lightweight and doesn't require Java or Spark installation.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "```bash\n",
    "pip install duckdb pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup DuckDB\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "\n",
    "# Create in-memory DuckDB connection\n",
    "con = duckdb.connect(\":memory:\")\n",
    "\n",
    "print(f\"DuckDB version: {duckdb.__version__}\")\n",
    "print(\"Connection established\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Demo Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer data\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE demo_customers AS\n",
    "SELECT * FROM (\n",
    "    VALUES\n",
    "    (1, 'Alice Johnson', 'alice@email.com', '2020-01-15', 'GOLD', 'US'),\n",
    "    (2, 'Bob Smith', 'bob@email.com', '2020-03-22', 'SILVER', 'US'),\n",
    "    (3, 'Carol White', 'carol@email.com', '2021-06-10', 'BRONZE', 'UK'),\n",
    "    (4, 'David Brown', 'david@email.com', '2021-08-05', 'GOLD', 'UK'),\n",
    "    (5, 'Eve Davis', 'eve@email.com', '2022-01-20', 'SILVER', 'CA'),\n",
    "    (6, 'Frank Miller', 'frank@email.com', '2022-04-12', 'BRONZE', 'CA'),\n",
    "    (7, 'Grace Wilson', 'grace@email.com', '2023-02-28', 'GOLD', 'US'),\n",
    "    (8, 'Henry Taylor', 'henry@email.com', '2023-05-15', 'SILVER', 'UK'),\n",
    "    (9, 'Ivy Anderson', 'ivy@email.com', '2023-07-01', 'BRONZE', 'US'),\n",
    "    (10, 'Jack Thomas', 'jack@email.com', '2023-09-10', 'GOLD', 'CA')\n",
    ") AS t(customer_id, customer_name, email, signup_date, tier, country)\n",
    "\"\"\")\n",
    "\n",
    "print(\"Created: demo_customers\")\n",
    "con.execute(\"SELECT * FROM demo_customers\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product data\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE demo_products AS\n",
    "SELECT * FROM (\n",
    "    VALUES\n",
    "    (101, 'Laptop Pro', 'Electronics', 1299.99, 0.15),\n",
    "    (102, 'Wireless Mouse', 'Electronics', 49.99, 0.10),\n",
    "    (103, 'USB-C Hub', 'Electronics', 79.99, 0.12),\n",
    "    (104, 'Desk Chair', 'Furniture', 299.99, 0.08),\n",
    "    (105, 'Standing Desk', 'Furniture', 599.99, 0.05),\n",
    "    (106, 'Monitor 27\"', 'Electronics', 449.99, 0.10),\n",
    "    (107, 'Keyboard', 'Electronics', 129.99, 0.15),\n",
    "    (108, 'Webcam HD', 'Electronics', 89.99, 0.20),\n",
    "    (109, 'Desk Lamp', 'Furniture', 59.99, 0.10),\n",
    "    (110, 'Cable Organizer', 'Accessories', 19.99, 0.25)\n",
    ") AS t(product_id, product_name, category, unit_price, discount_rate)\n",
    "\"\"\")\n",
    "\n",
    "print(\"Created: demo_products\")\n",
    "con.execute(\"SELECT * FROM demo_products\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order data\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE demo_orders AS\n",
    "SELECT * FROM (\n",
    "    VALUES\n",
    "    (1001, 1, '2024-01-05', 'COMPLETED', 'CREDIT_CARD'),\n",
    "    (1002, 2, '2024-01-08', 'COMPLETED', 'PAYPAL'),\n",
    "    (1003, 1, '2024-01-12', 'COMPLETED', 'CREDIT_CARD'),\n",
    "    (1004, 3, '2024-01-15', 'SHIPPED', 'CREDIT_CARD'),\n",
    "    (1005, 4, '2024-01-18', 'COMPLETED', 'BANK_TRANSFER'),\n",
    "    (1006, 5, '2024-01-22', 'COMPLETED', 'PAYPAL'),\n",
    "    (1007, 2, '2024-01-25', 'CANCELLED', 'CREDIT_CARD'),\n",
    "    (1008, 6, '2024-01-28', 'COMPLETED', 'CREDIT_CARD'),\n",
    "    (1009, 7, '2024-02-01', 'COMPLETED', 'PAYPAL'),\n",
    "    (1010, 8, '2024-02-05', 'SHIPPED', 'CREDIT_CARD'),\n",
    "    (1011, 1, '2024-02-08', 'COMPLETED', 'CREDIT_CARD'),\n",
    "    (1012, 9, '2024-02-12', 'COMPLETED', 'BANK_TRANSFER'),\n",
    "    (1013, 10, '2024-02-15', 'PENDING', 'PAYPAL'),\n",
    "    (1014, 3, '2024-02-18', 'COMPLETED', 'CREDIT_CARD'),\n",
    "    (1015, 4, '2024-02-22', 'COMPLETED', 'CREDIT_CARD')\n",
    ") AS t(order_id, customer_id, order_date, status, payment_method)\n",
    "\"\"\")\n",
    "\n",
    "print(\"Created: demo_orders\")\n",
    "con.execute(\"SELECT * FROM demo_orders\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order items data\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE demo_order_items AS\n",
    "SELECT * FROM (\n",
    "    VALUES\n",
    "    (1, 1001, 101, 1, 1299.99),\n",
    "    (2, 1001, 102, 2, 49.99),\n",
    "    (3, 1002, 104, 1, 299.99),\n",
    "    (4, 1002, 109, 2, 59.99),\n",
    "    (5, 1003, 106, 1, 449.99),\n",
    "    (6, 1003, 107, 1, 129.99),\n",
    "    (7, 1004, 101, 1, 1299.99),\n",
    "    (8, 1004, 103, 1, 79.99),\n",
    "    (9, 1005, 105, 1, 599.99),\n",
    "    (10, 1005, 104, 1, 299.99),\n",
    "    (11, 1006, 102, 3, 49.99),\n",
    "    (12, 1006, 110, 5, 19.99),\n",
    "    (13, 1007, 108, 1, 89.99),\n",
    "    (14, 1008, 106, 2, 449.99),\n",
    "    (15, 1008, 107, 1, 129.99),\n",
    "    (16, 1009, 101, 1, 1299.99),\n",
    "    (17, 1009, 102, 1, 49.99),\n",
    "    (18, 1010, 105, 1, 599.99),\n",
    "    (19, 1011, 103, 2, 79.99),\n",
    "    (20, 1011, 110, 3, 19.99),\n",
    "    (21, 1012, 104, 1, 299.99),\n",
    "    (22, 1013, 101, 1, 1299.99),\n",
    "    (23, 1014, 108, 2, 89.99),\n",
    "    (24, 1014, 109, 1, 59.99),\n",
    "    (25, 1015, 106, 1, 449.99)\n",
    ") AS t(item_id, order_id, product_id, quantity, unit_price)\n",
    "\"\"\")\n",
    "\n",
    "print(\"Created: demo_order_items\")\n",
    "con.execute(\"SELECT * FROM demo_order_items\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Demo Queries\n",
    "\n",
    "Two versions of a customer order summary query:\n",
    "- **QUERY_LEGACY**: Original implementation\n",
    "- **QUERY_NEW**: Refactored with intentional differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legacy Query - Original implementation\n",
    "QUERY_LEGACY = \"\"\"\n",
    "WITH order_totals AS (\n",
    "    SELECT\n",
    "        o.order_id,\n",
    "        o.customer_id,\n",
    "        o.order_date,\n",
    "        o.status,\n",
    "        SUM(oi.quantity * oi.unit_price) AS order_total\n",
    "    FROM demo_orders o\n",
    "    JOIN demo_order_items oi ON o.order_id = oi.order_id\n",
    "    WHERE o.status NOT IN ('CANCELLED', 'PENDING')\n",
    "    GROUP BY o.order_id, o.customer_id, o.order_date, o.status\n",
    "),\n",
    "\n",
    "customer_metrics AS (\n",
    "    SELECT\n",
    "        customer_id,\n",
    "        COUNT(DISTINCT order_id) AS order_count,\n",
    "        ROUND(SUM(order_total), 2) AS total_amount,\n",
    "        ROUND(AVG(order_total), 2) AS avg_order_value,\n",
    "        MIN(order_date) AS first_order_date,\n",
    "        MAX(order_date) AS last_order_date\n",
    "    FROM order_totals\n",
    "    GROUP BY customer_id\n",
    "),\n",
    "\n",
    "final AS (\n",
    "    SELECT\n",
    "        c.customer_id,\n",
    "        c.customer_name,\n",
    "        c.tier,\n",
    "        c.country,\n",
    "        cm.order_count,\n",
    "        cm.total_amount,\n",
    "        cm.avg_order_value,\n",
    "        cm.first_order_date,\n",
    "        cm.last_order_date,\n",
    "        CASE\n",
    "            WHEN cm.total_amount >= 2000 THEN 'Premium'\n",
    "            WHEN cm.total_amount >= 500 THEN 'High Value'\n",
    "            WHEN cm.total_amount >= 100 THEN 'Standard'\n",
    "            ELSE 'Low Value'\n",
    "        END AS customer_segment\n",
    "    FROM demo_customers c\n",
    "    JOIN customer_metrics cm ON c.customer_id = cm.customer_id\n",
    ")\n",
    "\n",
    "SELECT * FROM final\n",
    "ORDER BY customer_id\n",
    "\"\"\"\n",
    "\n",
    "print(\"QUERY_LEGACY defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Query - Refactored with intentional differences\n",
    "QUERY_NEW = \"\"\"\n",
    "WITH order_totals AS (\n",
    "    SELECT\n",
    "        o.order_id,\n",
    "        o.customer_id,\n",
    "        o.order_date,\n",
    "        o.status,\n",
    "        SUM(oi.quantity * oi.unit_price) AS order_total\n",
    "    FROM demo_orders o\n",
    "    JOIN demo_order_items oi ON o.order_id = oi.order_id\n",
    "    WHERE o.status NOT IN ('CANCELLED', 'PENDING')\n",
    "    GROUP BY o.order_id, o.customer_id, o.order_date, o.status\n",
    "),\n",
    "\n",
    "customer_metrics AS (\n",
    "    -- DIFFERENCE 1: Different rounding for total_amount (4 decimals vs 2)\n",
    "    -- DIFFERENCE 2: Different avg calculation (total/count instead of AVG)\n",
    "    SELECT\n",
    "        customer_id,\n",
    "        COUNT(DISTINCT order_id) AS order_count,\n",
    "        ROUND(SUM(order_total), 4) AS total_amount,\n",
    "        ROUND(SUM(order_total) / COUNT(DISTINCT order_id), 2) AS avg_order_value,\n",
    "        MIN(order_date) AS first_order_date,\n",
    "        MAX(order_date) AS last_order_date\n",
    "    FROM order_totals\n",
    "    GROUP BY customer_id\n",
    "),\n",
    "\n",
    "final AS (\n",
    "    -- DIFFERENCE 3: Different threshold for \\\"High Value\\\" (600 vs 500)\n",
    "    SELECT\n",
    "        c.customer_id,\n",
    "        c.customer_name,\n",
    "        c.tier,\n",
    "        c.country,\n",
    "        cm.order_count,\n",
    "        cm.total_amount,\n",
    "        cm.avg_order_value,\n",
    "        cm.first_order_date,\n",
    "        cm.last_order_date,\n",
    "        CASE\n",
    "            WHEN cm.total_amount >= 2000 THEN 'Premium'\n",
    "            WHEN cm.total_amount >= 600 THEN 'High Value'\n",
    "            WHEN cm.total_amount >= 100 THEN 'Standard'\n",
    "            ELSE 'Low Value'\n",
    "        END AS customer_segment\n",
    "    FROM demo_customers c\n",
    "    JOIN customer_metrics cm ON c.customer_id = cm.customer_id\n",
    ")\n",
    "\n",
    "SELECT * FROM final\n",
    "ORDER BY customer_id\n",
    "\"\"\"\n",
    "\n",
    "print(\"QUERY_NEW defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Queries Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Legacy Query Results:\")\n",
    "con.execute(QUERY_LEGACY).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"New Query Results:\")\n",
    "con.execute(QUERY_NEW).df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDB Comparison Utilities\n",
    "\n",
    "Below are the comparison utilities adapted for DuckDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DuckDB-based comparison utilities\n",
    "\n",
    "@dataclass\n",
    "class ComparisonConfig:\n",
    "    \"\"\"Configuration for CTE regression comparison.\"\"\"\n",
    "    prefix_a: str = \"a_\"\n",
    "    prefix_b: str = \"b_\"\n",
    "    only_common_ctes: bool = True\n",
    "    skip_columns: List[str] = None\n",
    "    sample_limit: int = 10\n",
    "    key_columns: Dict[str, List[str]] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.skip_columns is None:\n",
    "            self.skip_columns = []\n",
    "        if self.key_columns is None:\n",
    "            self.key_columns = {}\n",
    "\n",
    "\n",
    "def parse_ctes(sql: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Parse CTEs from a WITH clause SQL query.\"\"\"\n",
    "    sql_clean = re.sub(r'--.*$', '', sql, flags=re.MULTILINE)\n",
    "    sql_clean = re.sub(r'/\\*.*?\\*/', '', sql_clean, flags=re.DOTALL)\n",
    "    \n",
    "    if not re.match(r'\\s*WITH\\b', sql_clean, re.IGNORECASE):\n",
    "        return []\n",
    "    \n",
    "    ctes = []\n",
    "    pattern = r'(\\w+)\\s+AS\\s*\\('\n",
    "    matches = list(re.finditer(pattern, sql_clean, re.IGNORECASE))\n",
    "    \n",
    "    for match in matches:\n",
    "        cte_name = match.group(1)\n",
    "        start_paren = match.end() - 1\n",
    "        \n",
    "        depth = 1\n",
    "        pos = start_paren + 1\n",
    "        while pos < len(sql_clean) and depth > 0:\n",
    "            if sql_clean[pos] == '(':\n",
    "                depth += 1\n",
    "            elif sql_clean[pos] == ')':\n",
    "                depth -= 1\n",
    "            pos += 1\n",
    "        \n",
    "        cte_body = sql_clean[start_paren + 1:pos - 1].strip()\n",
    "        ctes.append((cte_name, cte_body))\n",
    "    \n",
    "    return ctes\n",
    "\n",
    "\n",
    "def get_final_select(sql: str) -> str:\n",
    "    \"\"\"Extract the final SELECT statement after all CTEs.\"\"\"\n",
    "    sql_clean = re.sub(r'--.*$', '', sql, flags=re.MULTILINE)\n",
    "    sql_clean = re.sub(r'/\\*.*?\\*/', '', sql_clean, flags=re.DOTALL)\n",
    "    \n",
    "    depth = 0\n",
    "    in_with = False\n",
    "    last_cte_end = 0\n",
    "    \n",
    "    for i, char in enumerate(sql_clean):\n",
    "        if sql_clean[i:i + 4].upper() == 'WITH':\n",
    "            in_with = True\n",
    "        elif char == '(':\n",
    "            depth += 1\n",
    "        elif char == ')':\n",
    "            depth -= 1\n",
    "            if depth == 0 and in_with:\n",
    "                last_cte_end = i + 1\n",
    "    \n",
    "    remainder = sql_clean[last_cte_end:].strip()\n",
    "    if remainder.startswith(','):\n",
    "        remainder = remainder[1:].strip()\n",
    "    \n",
    "    return remainder\n",
    "\n",
    "\n",
    "def register_ctes_as_views(sql: str, prefix: str) -> List[str]:\n",
    "    \"\"\"Register CTEs as views in DuckDB.\"\"\"\n",
    "    ctes = parse_ctes(sql)\n",
    "    if not ctes:\n",
    "        return []\n",
    "    \n",
    "    created_views = []\n",
    "    cte_names = [name for name, _ in ctes]\n",
    "    \n",
    "    for cte_name, cte_body in ctes:\n",
    "        adjusted_body = cte_body\n",
    "        for ref_name in cte_names:\n",
    "            pattern = rf'\\b{re.escape(ref_name)}\\b'\n",
    "            adjusted_body = re.sub(pattern, f'{prefix}{ref_name}', adjusted_body)\n",
    "        \n",
    "        view_name = f\"{prefix}{cte_name}\"\n",
    "        try:\n",
    "            con.execute(f\"CREATE OR REPLACE VIEW {view_name} AS {adjusted_body}\")\n",
    "            created_views.append(view_name)\n",
    "            print(f\"Created: {view_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed: {view_name} - {e}\")\n",
    "    \n",
    "    # Final SELECT\n",
    "    if 'final' not in [name.lower() for name in cte_names]:\n",
    "        final_select = get_final_select(sql)\n",
    "        if final_select and final_select.upper().startswith('SELECT'):\n",
    "            adjusted_final = final_select\n",
    "            for cte_name in cte_names:\n",
    "                pattern = rf'\\b{re.escape(cte_name)}\\b'\n",
    "                adjusted_final = re.sub(pattern, f'{prefix}{cte_name}', adjusted_final)\n",
    "            \n",
    "            view_name = f\"{prefix}final\"\n",
    "            try:\n",
    "                con.execute(f\"CREATE OR REPLACE VIEW {view_name} AS {adjusted_final}\")\n",
    "                created_views.append(view_name)\n",
    "                print(f\"Created: {view_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed: {view_name} - {e}\")\n",
    "    \n",
    "    return created_views\n",
    "\n",
    "\n",
    "def get_view_columns(view_name: str) -> List[str]:\n",
    "    \"\"\"Get column names from a view.\"\"\"\n",
    "    df = con.execute(f\"SELECT * FROM {view_name} LIMIT 0\").df()\n",
    "    return list(df.columns)\n",
    "\n",
    "\n",
    "def compare_views(view_a: str, view_b: str) -> Dict[str, Any]:\n",
    "    \"\"\"Compare two views and return differences.\"\"\"\n",
    "    # Row counts\n",
    "    count_a = con.execute(f\"SELECT COUNT(*) FROM {view_a}\").fetchone()[0]\n",
    "    count_b = con.execute(f\"SELECT COUNT(*) FROM {view_b}\").fetchone()[0]\n",
    "    \n",
    "    # Except counts (rows in A not in B)\n",
    "    except_a = con.execute(f\"\"\"\n",
    "        SELECT COUNT(*) FROM (\n",
    "            SELECT * FROM {view_a}\n",
    "            EXCEPT\n",
    "            SELECT * FROM {view_b}\n",
    "        )\n",
    "    \"\"\").fetchone()[0]\n",
    "    \n",
    "    # Except counts (rows in B not in A)\n",
    "    except_b = con.execute(f\"\"\"\n",
    "        SELECT COUNT(*) FROM (\n",
    "            SELECT * FROM {view_b}\n",
    "            EXCEPT\n",
    "            SELECT * FROM {view_a}\n",
    "        )\n",
    "    \"\"\").fetchone()[0]\n",
    "    \n",
    "    # Sample differences\n",
    "    sample_df = con.execute(f\"\"\"\n",
    "        SELECT * FROM (\n",
    "            SELECT 'SOURCE_ONLY' as _diff_source, * FROM {view_a}\n",
    "            EXCEPT\n",
    "            SELECT 'SOURCE_ONLY', * FROM {view_b}\n",
    "            \n",
    "            UNION ALL\n",
    "            \n",
    "            SELECT 'TARGET_ONLY' as _diff_source, * FROM {view_b}\n",
    "            EXCEPT\n",
    "            SELECT 'TARGET_ONLY', * FROM {view_a}\n",
    "        )\n",
    "        LIMIT 10\n",
    "    \"\"\").df()\n",
    "    \n",
    "    return {\n",
    "        \"rows_source\": count_a,\n",
    "        \"rows_target\": count_b,\n",
    "        \"except_source_vs_target\": except_a,\n",
    "        \"except_target_vs_source\": except_b,\n",
    "        \"total_differences\": except_a + except_b,\n",
    "        \"sample_df\": sample_df,\n",
    "        \"status\": \"PASS\" if (except_a + except_b) == 0 else \"FAIL\"\n",
    "    }\n",
    "\n",
    "\n",
    "def column_difference_summary(view_a: str, view_b: str, key_columns: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Summary of which columns differ most frequently.\"\"\"\n",
    "    all_columns = get_view_columns(view_a)\n",
    "    compare_columns = [c for c in all_columns if c not in key_columns]\n",
    "    \n",
    "    key_join = \" AND \".join([f\"a.{k} = b.{k}\" for k in key_columns])\n",
    "    \n",
    "    # Count differences per column\n",
    "    count_cases = []\n",
    "    for col in compare_columns:\n",
    "        count_cases.append(f\"\"\"\n",
    "        SUM(CASE WHEN a.\"{col}\" IS DISTINCT FROM b.\"{col}\" THEN 1 ELSE 0 END) AS \"{col}_diff\"\n",
    "        \"\"\")\n",
    "    \n",
    "    count_query = f\"\"\"\n",
    "    SELECT {', '.join(count_cases)}\n",
    "    FROM {view_a} a\n",
    "    INNER JOIN {view_b} b ON {key_join}\n",
    "    \"\"\"\n",
    "    \n",
    "    counts_row = con.execute(count_query).fetchone()\n",
    "    \n",
    "    data = []\n",
    "    for i, col in enumerate(compare_columns):\n",
    "        diff_count = counts_row[i]\n",
    "        if diff_count > 0:\n",
    "            data.append({\"column_name\": col, \"difference_count\": diff_count})\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    if not df.empty:\n",
    "        df = df.sort_values(\"difference_count\", ascending=False).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def full_regression_compare(\n",
    "    query_a: str,\n",
    "    query_b: str,\n",
    "    prefix_a: str = \"a_\",\n",
    "    prefix_b: str = \"b_\",\n",
    "    key_columns: Optional[Dict[str, List[str]]] = None,\n",
    "    config: ComparisonConfig = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Full CTE-by-CTE regression comparison.\"\"\"\n",
    "    config = config or ComparisonConfig()\n",
    "    key_columns = key_columns or config.key_columns or {}\n",
    "    \n",
    "    # Parse CTEs\n",
    "    ctes_a = parse_ctes(query_a)\n",
    "    ctes_b = parse_ctes(query_b)\n",
    "    \n",
    "    cte_names_a = [n for n, _ in ctes_a]\n",
    "    cte_names_b = [n for n, _ in ctes_b]\n",
    "    \n",
    "    common = set(cte_names_a) & set(cte_names_b)\n",
    "    only_a = set(cte_names_a) - common\n",
    "    only_b = set(cte_names_b) - common\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CTE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"CTEs in Query A: {len(cte_names_a)}\")\n",
    "    print(f\"CTEs in Query B: {len(cte_names_b)}\")\n",
    "    print(f\"Common CTEs:     {len(common)}\")\n",
    "    \n",
    "    if only_a:\n",
    "        print(f\"\\nCTEs ONLY in Query A (source): {sorted(only_a)}\")\n",
    "    if only_b:\n",
    "        print(f\"\\nCTEs ONLY in Query B (target): {sorted(only_b)}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Register views\n",
    "    print(\"\\n--- Registering Query A (Source) ---\")\n",
    "    register_ctes_as_views(query_a, prefix_a)\n",
    "    \n",
    "    print(\"\\n--- Registering Query B (Target) ---\")\n",
    "    register_ctes_as_views(query_b, prefix_b)\n",
    "    \n",
    "    # Show key_columns being used\n",
    "    if key_columns:\n",
    "        print(f\"\\nKey columns for column-level analysis: {key_columns}\")\n",
    "    \n",
    "    # Determine which CTEs to compare\n",
    "    if config.only_common_ctes:\n",
    "        ctes_to_compare = [n for n in cte_names_a if n in common] + [\"final\"]\n",
    "    else:\n",
    "        ctes_to_compare = cte_names_a + [\"final\"]\n",
    "    \n",
    "    # Compare each CTE\n",
    "    results = []\n",
    "    \n",
    "    for cte_name in ctes_to_compare:\n",
    "        if cte_name != \"final\" and cte_name not in common:\n",
    "            results.append({\n",
    "                \"cte_name\": cte_name,\n",
    "                \"status\": \"SKIPPED\",\n",
    "                \"reason\": \"Not present in target query\"\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        view_a = f\"{prefix_a}{cte_name}\"\n",
    "        view_b = f\"{prefix_b}{cte_name}\"\n",
    "        \n",
    "        # Look up key columns\n",
    "        keys = key_columns.get(cte_name)\n",
    "        if keys is None:\n",
    "            for kc_name, kc_cols in key_columns.items():\n",
    "                if kc_name.lower() == cte_name.lower():\n",
    "                    keys = kc_cols\n",
    "                    break\n",
    "        \n",
    "        try:\n",
    "            report = compare_views(view_a, view_b)\n",
    "            report[\"cte_name\"] = cte_name\n",
    "            report[\"source_view\"] = view_a\n",
    "            report[\"target_view\"] = view_b\n",
    "            \n",
    "            # Add column analysis if keys provided\n",
    "            if keys and report.get(\"total_differences\", 0) > 0:\n",
    "                print(f\"  -> Generating column analysis for {cte_name} using keys: {keys}\")\n",
    "                try:\n",
    "                    col_sum = column_difference_summary(view_a, view_b, keys)\n",
    "                    report[\"column_summary\"] = col_sum\n",
    "                    report[\"key_columns_used\"] = keys\n",
    "                    print(f\"  -> Found {len(col_sum)} columns with differences\")\n",
    "                except Exception as e:\n",
    "                    report[\"column_summary_error\"] = str(e)\n",
    "                    print(f\"  -> Column analysis failed: {e}\")\n",
    "            \n",
    "            results.append(report)\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"cte_name\": cte_name,\n",
    "                \"status\": \"ERROR\",\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_summary_table(results: List[Dict[str, Any]], show_details: bool = True) -> None:\n",
    "    \"\"\"Print a summary table of all CTE comparisons.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"REGRESSION SUMMARY\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"{'CTE Name':<25} {'Status':<8} {'Source Rows':>12} {'Target Rows':>12} {'Src-Tgt':>10} {'Tgt-Src':>10}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for r in results:\n",
    "        if r.get(\"status\") == \"ERROR\":\n",
    "            print(f\"{r['cte_name']:<25} {'ERROR':<8} {r.get('error', '')}\")\n",
    "        elif r.get(\"status\") == \"SKIPPED\":\n",
    "            print(f\"{r['cte_name']:<25} {'SKIPPED':<8} {r.get('reason', '')}\")\n",
    "        else:\n",
    "            icon = \"+\" if r[\"status\"] == \"PASS\" else \"x\"\n",
    "            print(f\"{r['cte_name']:<25} {icon} {r['status']:<6} \"\n",
    "                  f\"{r['rows_source']:>12,} {r['rows_target']:>12,} \"\n",
    "                  f\"{r['except_source_vs_target']:>10,} {r['except_target_vs_source']:>10,}\")\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "    passed = sum(1 for r in results if r.get(\"status\") == \"PASS\")\n",
    "    failed = sum(1 for r in results if r.get(\"status\") == \"FAIL\")\n",
    "    errors = sum(1 for r in results if r.get(\"status\") == \"ERROR\")\n",
    "    print(f\"Total: {len(results)} | Passed: {passed} | Failed: {failed} | Errors: {errors}\")\n",
    "    \n",
    "    if failed == 0 and errors == 0:\n",
    "        print(\"\\n*** ALL REGRESSION TESTS PASSED ***\")\n",
    "    else:\n",
    "        print(\"\\n*** REGRESSION FAILURES DETECTED ***\")\n",
    "        \n",
    "        if show_details:\n",
    "            failed_results = [r for r in results if r.get(\"status\") == \"FAIL\"]\n",
    "            for r in failed_results:\n",
    "                print(\"\\n\" + \"-\" * 100)\n",
    "                print(f\"FAILED: {r['cte_name']}\")\n",
    "                print(f\"  Source: {r.get('source_view', 'N/A')} | Target: {r.get('target_view', 'N/A')}\")\n",
    "                print(f\"  Rows in source only: {r.get('except_source_vs_target', 0):,}\")\n",
    "                print(f\"  Rows in target only: {r.get('except_target_vs_source', 0):,}\")\n",
    "                print(\"-\" * 100)\n",
    "                \n",
    "                # Show column summary if available\n",
    "                col_summary = r.get(\"column_summary\")\n",
    "                if col_summary is not None and not col_summary.empty:\n",
    "                    print(f\"\\nCOLUMNS CAUSING DIFFERENCES (sorted by impact):\")\n",
    "                    print(\"-\" * 60)\n",
    "                    print(f\"{'Column Name':<40} {'Diff Count':>15}\")\n",
    "                    print(\"-\" * 60)\n",
    "                    for _, row in col_summary.iterrows():\n",
    "                        print(f\"{row['column_name']:<40} {row['difference_count']:>15,}\")\n",
    "                    print(\"-\" * 60)\n",
    "                    print(f\"Total columns with differences: {len(col_summary)}\")\n",
    "                else:\n",
    "                    sample_df = r.get(\"sample_df\")\n",
    "                    if sample_df is not None and not sample_df.empty:\n",
    "                        print(f\"\\nTo see which COLUMNS cause differences, provide key_columns:\")\n",
    "                        print(f\"  key_columns={{'{r['cte_name']}': ['your_primary_key_column']}}\")\n",
    "                        print(f\"\\nSample differing rows:\")\n",
    "                        display(sample_df)\n",
    "                \n",
    "                if r.get(\"column_summary_error\"):\n",
    "                    print(f\"\\nColumn analysis error: {r['column_summary_error']}\")\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "\n",
    "\n",
    "def quick_compare(\n",
    "    query_a: str,\n",
    "    query_b: str,\n",
    "    key_columns: Optional[Dict[str, List[str]]] = None,\n",
    "    config: ComparisonConfig = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Quick comparison with summary output.\"\"\"\n",
    "    results = full_regression_compare(query_a, query_b, key_columns=key_columns, config=config)\n",
    "    print_summary_table(results)\n",
    "    return results\n",
    "\n",
    "\n",
    "def investigate_column(\n",
    "    view_a: str,\n",
    "    view_b: str,\n",
    "    key_columns: List[str],\n",
    "    column_name: str,\n",
    "    limit: int = 20\n",
    ") -> None:\n",
    "    \"\"\"Deep investigation of a specific column's differences.\"\"\"\n",
    "    key_join = \" AND \".join([f\"a.{k} = b.{k}\" for k in key_columns])\n",
    "    key_str = \", \".join([f\"a.{k}\" for k in key_columns])\n",
    "    \n",
    "    # Count\n",
    "    count_sql = f\"\"\"\n",
    "    SELECT COUNT(*) as diff_count\n",
    "    FROM {view_a} a\n",
    "    INNER JOIN {view_b} b ON {key_join}\n",
    "    WHERE a.\"{column_name}\" IS DISTINCT FROM b.\"{column_name}\"\n",
    "    \"\"\"\n",
    "    diff_count = con.execute(count_sql).fetchone()[0]\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"COLUMN INVESTIGATION: {column_name}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total rows with different values: {diff_count:,}\")\n",
    "    \n",
    "    # Distinct value pairs\n",
    "    pairs_sql = f\"\"\"\n",
    "    SELECT\n",
    "        a.\"{column_name}\" AS source_value,\n",
    "        b.\"{column_name}\" AS target_value,\n",
    "        COUNT(*) AS occurrence_count\n",
    "    FROM {view_a} a\n",
    "    INNER JOIN {view_b} b ON {key_join}\n",
    "    WHERE a.\"{column_name}\" IS DISTINCT FROM b.\"{column_name}\"\n",
    "    GROUP BY a.\"{column_name}\", b.\"{column_name}\"\n",
    "    ORDER BY occurrence_count DESC\n",
    "    LIMIT 20\n",
    "    \"\"\"\n",
    "    print(\"\\nDistinct value pairs (source -> target):\")\n",
    "    display(con.execute(pairs_sql).df())\n",
    "    \n",
    "    # Sample rows\n",
    "    sample_sql = f\"\"\"\n",
    "    SELECT\n",
    "        {key_str},\n",
    "        a.\"{column_name}\" AS source_{column_name},\n",
    "        b.\"{column_name}\" AS target_{column_name}\n",
    "    FROM {view_a} a\n",
    "    INNER JOIN {view_b} b ON {key_join}\n",
    "    WHERE a.\"{column_name}\" IS DISTINCT FROM b.\"{column_name}\"\n",
    "    LIMIT {limit}\n",
    "    \"\"\"\n",
    "    print(f\"\\nSample rows (limit {limit}):\")\n",
    "    display(con.execute(sample_sql).df())\n",
    "\n",
    "\n",
    "print(\"DuckDB comparison utilities loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Regression Analysis\n",
    "\n",
    "Compare the legacy and new queries to identify differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick comparison with column-level analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"RUNNING QUICK COMPARE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = quick_compare(\n",
    "    QUERY_LEGACY,\n",
    "    QUERY_NEW,\n",
    "    key_columns={\n",
    "        \"order_totals\": [\"order_id\"],\n",
    "        \"customer_metrics\": [\"customer_id\"],\n",
    "        \"final\": [\"customer_id\"]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate Specific Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INVESTIGATE: customer_segment column\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "investigate_column(\n",
    "    \"a_final\",\n",
    "    \"b_final\",\n",
    "    key_columns=[\"customer_id\"],\n",
    "    column_name=\"customer_segment\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INVESTIGATE: total_amount column\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "investigate_column(\n",
    "    \"a_final\",\n",
    "    \"b_final\",\n",
    "    key_columns=[\"customer_id\"],\n",
    "    column_name=\"total_amount\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Query Test (Should Pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query that should match the legacy query exactly\n",
    "QUERY_MATCHING = \"\"\"\n",
    "WITH order_totals AS (\n",
    "    SELECT\n",
    "        o.order_id,\n",
    "        o.customer_id,\n",
    "        o.order_date,\n",
    "        o.status,\n",
    "        SUM(oi.quantity * oi.unit_price) AS order_total\n",
    "    FROM demo_orders o\n",
    "    JOIN demo_order_items oi ON o.order_id = oi.order_id\n",
    "    WHERE o.status NOT IN ('CANCELLED', 'PENDING')\n",
    "    GROUP BY o.order_id, o.customer_id, o.order_date, o.status\n",
    "),\n",
    "\n",
    "customer_metrics AS (\n",
    "    SELECT\n",
    "        customer_id,\n",
    "        COUNT(DISTINCT order_id) AS order_count,\n",
    "        ROUND(SUM(order_total), 2) AS total_amount,\n",
    "        ROUND(AVG(order_total), 2) AS avg_order_value,\n",
    "        MIN(order_date) AS first_order_date,\n",
    "        MAX(order_date) AS last_order_date\n",
    "    FROM order_totals\n",
    "    GROUP BY customer_id\n",
    "),\n",
    "\n",
    "final AS (\n",
    "    SELECT\n",
    "        c.customer_id,\n",
    "        c.customer_name,\n",
    "        c.tier,\n",
    "        c.country,\n",
    "        cm.order_count,\n",
    "        cm.total_amount,\n",
    "        cm.avg_order_value,\n",
    "        cm.first_order_date,\n",
    "        cm.last_order_date,\n",
    "        CASE\n",
    "            WHEN cm.total_amount >= 2000 THEN 'Premium'\n",
    "            WHEN cm.total_amount >= 500 THEN 'High Value'\n",
    "            WHEN cm.total_amount >= 100 THEN 'Standard'\n",
    "            ELSE 'Low Value'\n",
    "        END AS customer_segment\n",
    "    FROM demo_customers c\n",
    "    JOIN customer_metrics cm ON c.customer_id = cm.customer_id\n",
    ")\n",
    "\n",
    "SELECT * FROM final\n",
    "ORDER BY customer_id\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MATCHING QUERY TEST (should PASS)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_match = quick_compare(\n",
    "    QUERY_LEGACY,\n",
    "    QUERY_MATCHING,\n",
    "    key_columns={\"final\": [\"customer_id\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This demo showed:\n",
    "\n",
    "1. **DuckDB as backend** - No Java/Spark required, just `pip install duckdb`\n",
    "2. **Same functionality** - CTE parsing, view registration, EXCEPT-based comparison\n",
    "3. **Column-level analysis** - Identifies exactly which columns cause differences\n",
    "4. **Deep investigation** - `investigate_column()` shows value pairs and samples\n",
    "5. **Pandas integration** - Results returned as DataFrames for easy analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "print(\"\\nDemo completed!\")\n",
    "print(\"\\nTo close connection, run: con.close()\")\n",
    "\n",
    "# Uncomment to close:\n",
    "# con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
